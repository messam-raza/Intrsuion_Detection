{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67094bc9",
   "metadata": {},
   "source": [
    "# IoT Intrusion Detection System â€“ Full Pipeline (Colab Ready)\n",
    "\n",
    "This notebook implements a **complete IoT IDS pipeline** based on the architecture you described:\n",
    "\n",
    "1. **Data loading from Google Drive**\n",
    "2. **Data preprocessing & cleaning**\n",
    "3. **Exploratory data analysis (EDA)** for class imbalance etc.\n",
    "4. **SMOTE** for handling class imbalance\n",
    "5. **Feature scaling (MinMaxScaler)**\n",
    "6. **Hybrid feature selection**  \n",
    "   - Binary Grey Wolf Optimizer (**BGWO**)  \n",
    "   - **RFEâ€“XGBoost** for fine-grained feature ranking\n",
    "7. **Hyperparameter optimization** of XGBoost using **Bayesian Optimization (BO-TPE via Hyperopt)**\n",
    "8. **Final XGBoost training & evaluation**\n",
    "9. **Checkpoint saving** (model, scaler, label encoder, feature masks, metrics, hyperparameters) to Google Drive\n",
    "\n",
    "> ðŸ”§ **Important**: You must update the `DATASETS` dictionary (paths and label column names) to match your actual dataset files in Google Drive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2506d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0. Install Dependencies\n",
    "# =========================\n",
    "# Run this cell once when you start a new Colab session.\n",
    "\n",
    "!pip install xgboost hyperopt imbalanced-learn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aa8d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1. Imports & Basic Setup\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7, 5)\n",
    "plt.rcParams['axes.grid'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90153488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# 2. Mount Google Drive & Dataset Configs\n",
    "# =======================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Root folder where you will store ALL datasets\n",
    "DATA_ROOT = \"/content/drive/MyDrive/IoT_IDS_datasets\"\n",
    "\n",
    "# Root folder for saving checkpoints (models, scalers, etc.)\n",
    "CHECKPOINT_ROOT = \"/content/drive/MyDrive/IoT_IDS_checkpoints\"\n",
    "os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# DATASET CONFIGURATIONS\n",
    "# ---------------------------\n",
    "# ðŸ”´ IMPORTANT:\n",
    "# 1. Update `path` to your actual CSV file in Drive.\n",
    "# 2. Set `label_col` to the correct label/target column.\n",
    "# 3. Optionally use `drop_cols` to remove ID or timestamp columns.\n",
    "# 4. Optionally define `binary_mapping` for binary experiments (normal vs attack).\n",
    "\n",
    "DATASETS = {\n",
    "    \"N_BaIoT\": {\n",
    "        \"path\": f\"{DATA_ROOT}/N_BaIoT_sample.csv\",  # TODO: change filename\n",
    "        \"label_col\": \"label\",                       # TODO: change to actual label column\n",
    "        \"drop_cols\": [],                            # e.g., ['id', 'timestamp']\n",
    "        \"binary_mapping\": None                      # or a dict mapping original labels to 'normal'/'attack'\n",
    "    },\n",
    "    \"BoT_IoT\": {\n",
    "        \"path\": f\"{DATA_ROOT}/BoT_IoT_sample.csv\",\n",
    "        \"label_col\": \"label\",\n",
    "        \"drop_cols\": [],\n",
    "        \"binary_mapping\": None\n",
    "    },\n",
    "    \"WUSTL_IIOT_2021\": {\n",
    "        \"path\": f\"{DATA_ROOT}/WUSTL_IIOT_2021_sample.csv\",\n",
    "        \"label_col\": \"label\",\n",
    "        \"drop_cols\": [],\n",
    "        \"binary_mapping\": None\n",
    "    },\n",
    "    \"WUSTL_EHMS_2020\": {\n",
    "        \"path\": f\"{DATA_ROOT}/WUSTL_EHMS_2020_sample.csv\",\n",
    "        \"label_col\": \"label\",\n",
    "        \"drop_cols\": [],\n",
    "        \"binary_mapping\": None\n",
    "    },\n",
    "    \"NSL_KDD\": {\n",
    "        \"path\": f\"{DATA_ROOT}/NSL_KDD_sample.csv\",\n",
    "        \"label_col\": \"label\",\n",
    "        \"drop_cols\": [],\n",
    "        \"binary_mapping\": None\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33325c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 3. Data Loading, EDA & Preprocessing\n",
    "# =====================================\n",
    "\n",
    "def load_dataset(config):\n",
    "    \"\"\"\n",
    "    Load dataset from CSV based on the configuration dictionary.\n",
    "    \"\"\"\n",
    "    path = config[\"path\"]\n",
    "    print(f\"[INFO] Loading dataset from: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"[INFO] Raw shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def explore_dataset(df, label_col):\n",
    "    \"\"\"\n",
    "    Basic exploratory data analysis (EDA):\n",
    "    - Show first rows\n",
    "    - Summary info\n",
    "    - Class distribution for labels\n",
    "    - Missing values per column\n",
    "    \"\"\"\n",
    "    print(\"\\n[EDA] Head:\")\n",
    "    display(df.head())\n",
    "\n",
    "    print(\"\\n[EDA] Info:\")\n",
    "    print(df.info())\n",
    "\n",
    "    print(\"\\n[EDA] Label distribution:\")\n",
    "    if label_col in df.columns:\n",
    "        print(df[label_col].value_counts())\n",
    "        df[label_col].value_counts().plot(kind='bar', title='Label Distribution')\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Label column '{label_col}' not found in dataframe.\")\n",
    "\n",
    "    print(\"\\n[EDA] Missing values per column:\")\n",
    "    print(df.isna().sum().sort_values(ascending=False).head(20))\n",
    "\n",
    "\n",
    "def preprocess_dataset(df, label_col, drop_cols=None, binary_mapping=None):\n",
    "    \"\"\"\n",
    "    Full preprocessing pipeline:\n",
    "\n",
    "    1. Drop unwanted columns (IDs, timestamps, etc.).\n",
    "    2. Drop rows where label is missing.\n",
    "    3. Optional binary mapping (e.g., normal vs attack).\n",
    "    4. Drop duplicate rows.\n",
    "    5. Separate features (X) and labels (y).\n",
    "    6. Handle numerical + categorical features:\n",
    "       - Convert obvious numeric strings to numeric.\n",
    "       - One-hot encode remaining categorical columns.\n",
    "    7. Handle missing values (fill with 0 for simplicity and robustness).\n",
    "    8. Label-encode target variable.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) Drop columns if requested\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "    # 2) Drop rows where label is missing\n",
    "    df = df.dropna(subset=[label_col])\n",
    "\n",
    "    # 3) Optional binary mapping\n",
    "    if binary_mapping is not None:\n",
    "        df[label_col] = df[label_col].map(binary_mapping)\n",
    "        # Drop rows with unmapped labels\n",
    "        df = df.dropna(subset=[label_col])\n",
    "\n",
    "    # 4) Remove duplicates\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # 5) Separate features and labels\n",
    "    y_raw = df[label_col]\n",
    "    X_raw = df.drop(columns=[label_col])\n",
    "\n",
    "    # 6) Convert obviously numeric strings to numeric\n",
    "    for col in X_raw.columns:\n",
    "        if X_raw[col].dtype == 'object':\n",
    "            try:\n",
    "                X_raw[col] = pd.to_numeric(X_raw[col])\n",
    "            except Exception:\n",
    "                # keep as object for one-hot encoding later\n",
    "                pass\n",
    "\n",
    "    # Identify categorical (object) columns\n",
    "    cat_cols = X_raw.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    num_cols = [c for c in X_raw.columns if c not in cat_cols]\n",
    "\n",
    "    # Handle missing values separately for numeric and categorical\n",
    "    if num_cols:\n",
    "        X_raw[num_cols] = X_raw[num_cols].fillna(X_raw[num_cols].median())\n",
    "    if cat_cols:\n",
    "        X_raw[cat_cols] = X_raw[cat_cols].fillna(X_raw[cat_cols].mode().iloc[0])\n",
    "\n",
    "    # One-hot encode categorical columns\n",
    "    X = pd.get_dummies(X_raw, columns=cat_cols, drop_first=True)\n",
    "\n",
    "    # As a safety net, fill any remaining NaNs\n",
    "    X = X.fillna(0)\n",
    "\n",
    "    # 7) Label encode target\n",
    "    y_encoder = LabelEncoder()\n",
    "    y = y_encoder.fit_transform(y_raw)\n",
    "\n",
    "    print(f\"[PREPROCESS] After preprocessing: X shape = {X.shape}, y shape = {y.shape}\")\n",
    "    print(f\"[PREPROCESS] Number of classes: {len(np.unique(y))}\")\n",
    "\n",
    "    return X.values, y, y_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc67978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 4. SMOTE Oversampling & Feature Scaling\n",
    "# =========================================\n",
    "\n",
    "def apply_smote_and_scale(X_train, y_train, X_test, random_state=42):\n",
    "    \"\"\"\n",
    "    Apply SMOTE to handle class imbalance, then MinMax scaling.\n",
    "\n",
    "    Returns:\n",
    "    - X_res_scaled: Resampled and scaled training features\n",
    "    - y_res: Resampled labels\n",
    "    - X_test_scaled: Scaled test features\n",
    "    - scaler: Fitted MinMaxScaler (for reuse on new data)\n",
    "    \"\"\"\n",
    "    sm = SMOTE(random_state=random_state)\n",
    "    X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "    print(f\"[SMOTE] Train before: {X_train.shape}, after: {X_res.shape}\")\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_res_scaled = scaler.fit_transform(X_res)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_res_scaled, y_res, X_test_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ccc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 5. Binary Grey Wolf Optimizer (BGWO)\n",
    "# ====================================\n",
    "\n",
    "def evaluate_feature_subset(mask, X, y, random_state=42):\n",
    "    \"\"\"\n",
    "    Fitness function for BGWO:\n",
    "    - Train a lightweight XGBoost model on the selected features.\n",
    "    - Use validation accuracy.\n",
    "    - Slightly penalize using too many features.\n",
    "    \"\"\"\n",
    "    if mask.sum() == 0:\n",
    "        return 0.0  # invalid: no features\n",
    "\n",
    "    X_sub = X[:, mask]\n",
    "\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_sub, y, test_size=0.3, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Lightweight XGBoost for fitness evaluation (fast but effective)\n",
    "    clf = XGBClassifier(\n",
    "        n_estimators=50,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='multi:softmax' if len(np.unique(y)) > 2 else 'binary:logistic',\n",
    "        eval_metric='mlogloss' if len(np.unique(y)) > 2 else 'logloss',\n",
    "        tree_method='hist',\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    # Penalty for many features\n",
    "    feat_ratio = mask.sum() / len(mask)\n",
    "    fitness = 0.99 * acc + 0.01 * (1.0 - feat_ratio)\n",
    "    return fitness\n",
    "\n",
    "\n",
    "def bgwo_feature_selection(\n",
    "    X, y,\n",
    "    num_wolves=10,\n",
    "    max_iter=15,\n",
    "    subsample=8000,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Binary Grey Wolf Optimization for feature selection.\n",
    "\n",
    "    - X, y: Training data (scaled).\n",
    "    - num_wolves: population size.\n",
    "    - max_iter: number of iterations.\n",
    "    - subsample: if dataset is huge, we use a subset to speed up fitness evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - best_mask: boolean array indicating selected features.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # Subsample to speed up fitness evaluation on large datasets\n",
    "    if n_samples > subsample:\n",
    "        idx = np.random.choice(n_samples, subsample, replace=False)\n",
    "        X_eval = X[idx]\n",
    "        y_eval = y[idx]\n",
    "    else:\n",
    "        X_eval, y_eval = X, y\n",
    "\n",
    "    # Initialize wolves in continuous space [0, 1]\n",
    "    positions = np.random.rand(num_wolves, n_features)\n",
    "\n",
    "    # Sigmoid-based binarization\n",
    "    def continuous_to_binary(pos):\n",
    "        s = 1 / (1 + np.exp(-10 * (pos - 0.5)))\n",
    "        rand_mat = np.random.rand(*pos.shape)\n",
    "        return (s > rand_mat).astype(int)\n",
    "\n",
    "    # Initialize alpha, beta, delta wolves\n",
    "    alpha_pos = None\n",
    "    beta_pos = None\n",
    "    delta_pos = None\n",
    "    alpha_score = -np.inf\n",
    "    beta_score = -np.inf\n",
    "    delta_score = -np.inf\n",
    "\n",
    "    for iter_idx in range(max_iter):\n",
    "        # Parameter 'a' decreases linearly from 2 to 0\n",
    "        a = 2 - 2 * (iter_idx / max_iter)\n",
    "\n",
    "        for i in range(num_wolves):\n",
    "            bin_mask = continuous_to_binary(positions[i])\n",
    "            fitness = evaluate_feature_subset(bin_mask.astype(bool), X_eval, y_eval, random_state)\n",
    "\n",
    "            # Update alpha, beta, delta\n",
    "            if fitness > alpha_score:\n",
    "                delta_score, delta_pos = beta_score, beta_pos\n",
    "                beta_score, beta_pos = alpha_score, alpha_pos\n",
    "                alpha_score, alpha_pos = fitness, positions[i].copy()\n",
    "            elif fitness > beta_score:\n",
    "                delta_score, delta_pos = beta_score, beta_pos\n",
    "                beta_score, beta_pos = fitness, positions[i].copy()\n",
    "            elif fitness > delta_score:\n",
    "                delta_score, delta_pos = fitness, positions[i].copy()\n",
    "\n",
    "        # Position update for all wolves\n",
    "        for i in range(num_wolves):\n",
    "            for j in range(n_features):\n",
    "                r1, r2 = np.random.rand(), np.random.rand()\n",
    "                A1 = 2 * a * r1 - a\n",
    "                C1 = 2 * r2\n",
    "                D_alpha = abs(C1 * alpha_pos[j] - positions[i, j])\n",
    "                X1 = alpha_pos[j] - A1 * D_alpha\n",
    "\n",
    "                r1, r2 = np.random.rand(), np.random.rand()\n",
    "                A2 = 2 * a * r1 - a\n",
    "                C2 = 2 * r2\n",
    "                D_beta = abs(C2 * beta_pos[j] - positions[i, j])\n",
    "                X2 = beta_pos[j] - A2 * D_beta\n",
    "\n",
    "                r1, r2 = np.random.rand(), np.random.rand()\n",
    "                A3 = 2 * a * r1 - a\n",
    "                C3 = 2 * r2\n",
    "                D_delta = abs(C3 * delta_pos[j] - positions[i, j])\n",
    "                X3 = delta_pos[j] - A3 * D_delta\n",
    "\n",
    "                positions[i, j] = (X1 + X2 + X3) / 3.0\n",
    "\n",
    "        # Clip positions to [0, 1]\n",
    "        positions = np.clip(positions, 0, 1)\n",
    "\n",
    "        print(f\"[BGWO] Iter {iter_idx + 1}/{max_iter} - best fitness: {alpha_score:.6f}\")\n",
    "\n",
    "    best_binary = continuous_to_binary(alpha_pos).astype(bool)\n",
    "    print(f\"[BGWO] Selected {best_binary.sum()} / {n_features} features\")\n",
    "    return best_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab80b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 6. RFE-XGBoost (Refined Feature Selection)\n",
    "# =============================================\n",
    "\n",
    "def rfe_xgboost_feature_selection(X, y, initial_mask, random_state=42):\n",
    "    \"\"\"\n",
    "    Second-stage feature selection:\n",
    "\n",
    "    1. Restrict features to those selected by BGWO.\n",
    "    2. Apply RFE (Recursive Feature Elimination) using XGBoost as base estimator.\n",
    "    3. Return a final boolean mask over ALL original features.\n",
    "\n",
    "    Parameters:\n",
    "    - X, y: training data (after SMOTE & scaling).\n",
    "    - initial_mask: boolean mask of features selected by BGWO.\n",
    "\n",
    "    Returns:\n",
    "    - final_mask: boolean mask over all original features.\n",
    "    \"\"\"\n",
    "    selected_indices = np.where(initial_mask)[0]\n",
    "    X_reduced = X[:, selected_indices]\n",
    "\n",
    "    base_clf = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='multi:softmax' if len(np.unique(y)) > 2 else 'binary:logistic',\n",
    "        eval_metric='mlogloss' if len(np.unique(y)) > 2 else 'logloss',\n",
    "        tree_method='hist',\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "\n",
    "    # Keep about 50% of BGWO-selected features (tunable)\n",
    "    n_features_bgwo = X_reduced.shape[1]\n",
    "    n_to_select = max(5, n_features_bgwo // 2)\n",
    "\n",
    "    print(f\"[RFE] Running RFE on {n_features_bgwo} features, selecting {n_to_select}...\")\n",
    "    rfe = RFE(\n",
    "        estimator=base_clf,\n",
    "        n_features_to_select=n_to_select,\n",
    "        step=0.1\n",
    "    )\n",
    "    rfe.fit(X_reduced, y)\n",
    "    support_reduced = rfe.support_  # mask among BGWO features\n",
    "\n",
    "    # Map back to full feature space\n",
    "    final_mask = np.zeros(X.shape[1], dtype=bool)\n",
    "    final_mask[selected_indices[support_reduced]] = True\n",
    "\n",
    "    print(f\"[RFE] Final selected features: {final_mask.sum()} / {X.shape[1]}\")\n",
    "    return final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5551a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# 7. Hyperparameter Optimization (BO-TPE) & Final Model Evaluation\n",
    "# ==================================================================\n",
    "\n",
    "def bo_tpe_optimize_xgboost(X, y, max_evals=25, random_state=42):\n",
    "    \"\"\"\n",
    "    Bayesian Optimization (Tree-structured Parzen Estimator - TPE)\n",
    "    to tune key XGBoost hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "    - best_params: dictionary with best n_estimators, max_depth, learning_rate\n",
    "    \"\"\"\n",
    "    n_classes = len(np.unique(y))\n",
    "    objective_type = 'multi:softprob' if n_classes > 2 else 'binary:logistic'\n",
    "    eval_metric = 'mlogloss' if n_classes > 2 else 'logloss'\n",
    "\n",
    "    def objective(params):\n",
    "        params = dict(params)\n",
    "\n",
    "        clf = XGBClassifier(\n",
    "            n_estimators=int(params['n_estimators']),\n",
    "            max_depth=int(params['max_depth']),\n",
    "            learning_rate=float(params['learning_rate']),\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            objective=objective_type,\n",
    "            eval_metric=eval_metric,\n",
    "            tree_method='hist',\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            verbosity=0\n",
    "        )\n",
    "\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "        scores = cross_val_score(clf, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "        acc = scores.mean()\n",
    "        loss = 1.0 - acc\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'status': STATUS_OK,\n",
    "            'acc': acc\n",
    "        }\n",
    "\n",
    "    search_space = {\n",
    "        'n_estimators': hp.quniform('n_estimators', 50, 200, 10),\n",
    "        'max_depth': hp.quniform('max_depth', 3, 12, 1),\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5))\n",
    "    }\n",
    "\n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fn=objective,\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_evals,\n",
    "        trials=trials,\n",
    "        rstate=np.random.RandomState(random_state)\n",
    "    )\n",
    "\n",
    "    best_params = {\n",
    "        'n_estimators': int(best['n_estimators']),\n",
    "        'max_depth': int(best['max_depth']),\n",
    "        'learning_rate': float(best['learning_rate'])\n",
    "    }\n",
    "    print(f\"[BO-TPE] Best params: {best_params}\")\n",
    "    return best_params\n",
    "\n",
    "\n",
    "def train_evaluate_xgboost(X_train, y_train, X_test, y_test, params, random_state=42):\n",
    "    \"\"\"\n",
    "    Train final XGBoost with optimized hyperparameters and evaluate on test set.\n",
    "\n",
    "    Returns:\n",
    "    - clf: trained XGBClassifier\n",
    "    - metrics: (accuracy, precision, recall, f1)\n",
    "    \"\"\"\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    objective_type = 'multi:softmax' if n_classes > 2 else 'binary:logistic'\n",
    "    eval_metric = 'mlogloss' if n_classes > 2 else 'logloss'\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        max_depth=params['max_depth'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=objective_type,\n",
    "        eval_metric=eval_metric,\n",
    "        tree_method='hist',\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    print(\"\\n[RESULTS]\")\n",
    "    print(f\"Accuracy : {acc:.6f}\")\n",
    "    print(f\"Precision: {prec:.6f}\")\n",
    "    print(f\"Recall   : {rec:.6f}\")\n",
    "    print(f\"F1-score : {f1:.6f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "    return clf, (acc, prec, rec, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# 8. Checkpoint Saving (for Next Phases)\n",
    "# =======================================\n",
    "\n",
    "def save_checkpoints(\n",
    "    save_dir,\n",
    "    scaler,\n",
    "    label_encoder,\n",
    "    bgwo_mask,\n",
    "    final_mask,\n",
    "    best_params,\n",
    "    metrics,\n",
    "    model\n",
    "):\n",
    "    \"\"\"\n",
    "    Save all important artifacts for later reuse.\n",
    "\n",
    "    Files created inside save_dir:\n",
    "    - scaler.joblib          : MinMaxScaler\n",
    "    - label_encoder.joblib   : LabelEncoder for target\n",
    "    - bgwo_mask.npy          : Boolean mask after BGWO\n",
    "    - final_mask.npy         : Boolean mask after RFE (final features)\n",
    "    - best_params.json       : Best hyperparameters found by BO-TPE\n",
    "    - metrics.json           : Accuracy, Precision, Recall, F1\n",
    "    - xgb_model.json         : Trained XGBoost model (booster format)\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Scaler & label encoder\n",
    "    joblib.dump(scaler, os.path.join(save_dir, \"scaler.joblib\"))\n",
    "    joblib.dump(label_encoder, os.path.join(save_dir, \"label_encoder.joblib\"))\n",
    "\n",
    "    # Feature masks\n",
    "    np.save(os.path.join(save_dir, \"bgwo_mask.npy\"), bgwo_mask.astype(bool))\n",
    "    np.save(os.path.join(save_dir, \"final_mask.npy\"), final_mask.astype(bool))\n",
    "\n",
    "    # Hyperparameters & metrics\n",
    "    with open(os.path.join(save_dir, \"best_params.json\"), \"w\") as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": float(metrics[0]),\n",
    "        \"precision\": float(metrics[1]),\n",
    "        \"recall\": float(metrics[2]),\n",
    "        \"f1_score\": float(metrics[3]),\n",
    "    }\n",
    "    with open(os.path.join(save_dir, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics_dict, f, indent=4)\n",
    "\n",
    "    # Model (XGBoost)\n",
    "    model_path = os.path.join(save_dir, \"xgb_model.json\")\n",
    "    model.save_model(model_path)\n",
    "\n",
    "    print(f\"[CHECKPOINT] Saved all artifacts to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a6c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 9. Full Pipeline (One Dataset)\n",
    "# ================================\n",
    "\n",
    "def run_full_pipeline(\n",
    "    dataset_name,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    bgwo_wolves=10,\n",
    "    bgwo_iters=10,\n",
    "    bgwo_subsample=8000,\n",
    "    bo_evals=20,\n",
    "    save_checkpoints_flag=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the complete IDS pipeline on a given dataset.\n",
    "\n",
    "    Steps:\n",
    "    1. Load dataset.\n",
    "    2. EDA (optional, you can call explore_dataset separately).\n",
    "    3. Preprocess (cleaning, encoding).\n",
    "    4. Train-test split.\n",
    "    5. SMOTE + MinMax scaling.\n",
    "    6. BGWO feature selection.\n",
    "    7. RFE-XGBoost feature selection.\n",
    "    8. Hyperparameter optimization with BO-TPE.\n",
    "    9. Final XGBoost training & evaluation.\n",
    "    10. Save checkpoints to Google Drive (optional).\n",
    "\n",
    "    Returns a dictionary containing:\n",
    "    - model\n",
    "    - metrics\n",
    "    - label_encoder\n",
    "    - scaler\n",
    "    - feature_mask (final mask)\n",
    "    - bgwo_mask\n",
    "    - best_params\n",
    "    - checkpoint_dir (if saving enabled)\n",
    "    \"\"\"\n",
    "    assert dataset_name in DATASETS, f\"Unknown dataset: {dataset_name}\"\n",
    "\n",
    "    cfg = DATASETS[dataset_name]\n",
    "\n",
    "    # Create checkpoint folder for this dataset\n",
    "    save_dir = os.path.join(CHECKPOINT_ROOT, dataset_name)\n",
    "    if save_checkpoints_flag:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 1) Load\n",
    "    df = load_dataset(cfg)\n",
    "\n",
    "    # Optional: EDA (uncomment if you want to see it each run)\n",
    "    # explore_dataset(df, cfg['label_col'])\n",
    "\n",
    "    # 2) Preprocess\n",
    "    X, y, y_encoder = preprocess_dataset(\n",
    "        df,\n",
    "        label_col=cfg[\"label_col\"],\n",
    "        drop_cols=cfg.get(\"drop_cols\", []),\n",
    "        binary_mapping=cfg.get(\"binary_mapping\", None)\n",
    "    )\n",
    "\n",
    "    # 3) Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    print(f\"[SPLIT] Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "    # 4) SMOTE + scaling\n",
    "    X_train_res, y_train_res, X_test_scaled, scaler = apply_smote_and_scale(\n",
    "        X_train, y_train, X_test, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 5) BGWO feature selection\n",
    "    bgwo_mask = bgwo_feature_selection(\n",
    "        X_train_res, y_train_res,\n",
    "        num_wolves=bgwo_wolves,\n",
    "        max_iter=bgwo_iters,\n",
    "        subsample=bgwo_subsample,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train_bgwo = X_train_res[:, bgwo_mask]\n",
    "    X_test_bgwo = X_test_scaled[:, bgwo_mask]\n",
    "    print(f\"[BGWO] After BGWO: Train {X_train_bgwo.shape}, Test {X_test_bgwo.shape}\")\n",
    "\n",
    "    # 6) RFE-XGBoost feature selection\n",
    "    final_mask = rfe_xgboost_feature_selection(\n",
    "        X_train_res, y_train_res, bgwo_mask, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train_fs = X_train_res[:, final_mask]\n",
    "    X_test_fs = X_test_scaled[:, final_mask]\n",
    "    print(f\"[FEATURES] Final selected: Train {X_train_fs.shape}, Test {X_test_fs.shape}\")\n",
    "\n",
    "    # 7) Hyperparameter optimization\n",
    "    best_params = bo_tpe_optimize_xgboost(\n",
    "        X_train_fs, y_train_res,\n",
    "        max_evals=bo_evals,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 8) Final training & evaluation\n",
    "    clf, metrics = train_evaluate_xgboost(\n",
    "        X_train_fs, y_train_res,\n",
    "        X_test_fs, y_test,\n",
    "        best_params,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 9) Save checkpoints\n",
    "    if save_checkpoints_flag:\n",
    "        save_checkpoints(\n",
    "            save_dir=save_dir,\n",
    "            scaler=scaler,\n",
    "            label_encoder=y_encoder,\n",
    "            bgwo_mask=bgwo_mask,\n",
    "            final_mask=final_mask,\n",
    "            best_params=best_params,\n",
    "            metrics=metrics,\n",
    "            model=clf\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"model\": clf,\n",
    "        \"metrics\": metrics,\n",
    "        \"label_encoder\": y_encoder,\n",
    "        \"scaler\": scaler,\n",
    "        \"feature_mask\": final_mask,\n",
    "        \"bgwo_mask\": bgwo_mask,\n",
    "        \"best_params\": best_params,\n",
    "        \"checkpoint_dir\": save_dir if save_checkpoints_flag else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626396b",
   "metadata": {},
   "source": [
    "## 10. Run the Full Pipeline on a Dataset\n",
    "\n",
    "- Make sure you have **updated the `DATASETS` dictionary** (paths + label column + optional binary mapping).\n",
    "- Then you can run the pipeline on any of the defined dataset keys, for example: `NSL_KDD`, `BoT_IoT`, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddff955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Example: Run on NSL_KDD\n",
    "# =======================\n",
    "# 1. Ensure `DATASETS[\"NSL_KDD\"][\"path\"]` and `label_col` are correct.\n",
    "# 2. Then run this cell.\n",
    "\n",
    "# Uncomment when ready:\n",
    "# result_nsl = run_full_pipeline(\n",
    "#     dataset_name=\"NSL_KDD\",\n",
    "#     test_size=0.2,\n",
    "#     random_state=42,\n",
    "#     bgwo_wolves=10,\n",
    "#     bgwo_iters=8,       # you can increase this for better search (more time)\n",
    "#     bgwo_subsample=8000,\n",
    "#     bo_evals=15,        # more evaluations -> better tuning (more time)\n",
    "#     save_checkpoints_flag=True\n",
    "# )\n",
    "# print(\"NSL-KDD metrics:\", result_nsl[\"metrics\"])\n",
    "# print(\"Checkpoints saved in:\", result_nsl[\"checkpoint_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b758d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Example: Run on BoT-IoT\n",
    "# =======================\n",
    "# Uncomment when you have configured DATASETS[\"BoT_IoT\"] correctly.\n",
    "\n",
    "# result_bot = run_full_pipeline(\n",
    "#     dataset_name=\"BoT_IoT\",\n",
    "#     test_size=0.2,\n",
    "#     random_state=42,\n",
    "#     bgwo_wolves=10,\n",
    "#     bgwo_iters=8,\n",
    "#     bgwo_subsample=8000,\n",
    "#     bo_evals=15,\n",
    "#     save_checkpoints_flag=True\n",
    "# )\n",
    "# print(\"BoT-IoT metrics:\", result_bot[\"metrics\"])\n",
    "# print(\"Checkpoints saved in:\", result_bot[\"checkpoint_dir\"])"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
