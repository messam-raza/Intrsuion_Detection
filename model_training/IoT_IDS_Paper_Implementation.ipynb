{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c9699fc",
   "metadata": {},
   "source": [
    "# IoT IDS â€“ BGWO + RFE-XGBoost Complete Pipeline\n",
    "\n",
    "This notebook implements the full intrusion detection pipeline with:\n",
    "\n",
    "- BGWO (Binary Grey Wolf Optimizer) + RFE-XGBoost feature selection  \n",
    "- BO-TPE (Optuna TPE sampler) for hyperparameter optimization  \n",
    "- Binary and multi-class classification (Normal vs Attack, plus 7-class attacks)  \n",
    "- SMOTE for class imbalance  \n",
    "- Attack-specific evaluation metrics  \n",
    "- Checkpoint saving (models, scalers, selected features, Optuna study)\n",
    "\n",
    "The notebook is self-contained and runs end-to-end on a **synthetic IoT dataset** so that you can execute it without any external files.\n",
    "\n",
    "To use your real datasets (N-BaIoT, BoT-IoT, WUSTL-IIoT-2021, WUSTL-EHMS-2020, NSL-KDD), replace the synthetic `df_syn` with your preprocessed merged dataframe and keep the rest of the pipeline unchanged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dac319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BGWO + RFE-XGBoost IoT IDS Pipeline\n",
    "# This notebook cell defines a complete, end-to-end implementation\n",
    "# that you can run in Google Colab. It includes:\n",
    "# - Synthetic example data (so the pipeline runs without external files)\n",
    "# - BGWO + RFE feature selection for XGBoost\n",
    "# - BO-TPE hyperparameter optimization using Optuna\n",
    "# - Binary and multi-class classification support\n",
    "# - SMOTE for class imbalance\n",
    "# - Attack-specific analysis\n",
    "# - Checkpoint saving for later reuse\n",
    "\n",
    "# =========================\n",
    "# 1. Install dependencies (for Colab / any Python env)\n",
    "# =========================\n",
    "\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def ensure_package(package_name: str):\n",
    "    try:\n",
    "        importlib.import_module(package_name)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "\n",
    "for pkg in [\"optuna\", \"imbalanced-learn\", \"xgboost\", \"joblib\"]:\n",
    "    ensure_package(pkg)\n",
    "\n",
    "# =========================\n",
    "# 2. Imports\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import joblib\n",
    "\n",
    "# For reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "# =========================\n",
    "# 3. Configuration\n",
    "# =========================\n",
    "\n",
    "# Directory where models, feature lists, and studies will be saved.\n",
    "# In Colab + Drive, you can do:\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# and then set:\n",
    "# CHECKPOINT_DIR = \"/content/drive/MyDrive/iot_ids_checkpoints\"\n",
    "\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Attack labels as described in your project\n",
    "ATTACK_LABELS = [\n",
    "    \"Normal\",\n",
    "    \"Data Tampering\",\n",
    "    \"Impersonation/Spoofing\",\n",
    "    \"DDoS/DoS\",\n",
    "    \"Silent Exfiltration\",\n",
    "    \"Backdoor\",\n",
    "    \"Reconnaissance\",\n",
    "]\n",
    "ATTACK_LABEL_TO_ID = {name: idx for idx, name in enumerate(ATTACK_LABELS)}\n",
    "ATTACK_ID_TO_LABEL = {idx: name for name, idx in enumerate(ATTACK_LABELS)}\n",
    "\n",
    "# =========================\n",
    "# 4. Synthetic dataset builder\n",
    "# =========================\n",
    "\n",
    "def build_synthetic_iot_dataset(\n",
    "    n_samples_per_class: int = 800,\n",
    "    n_features: int = 40,\n",
    "    random_state: int = RANDOM_STATE,\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Build a synthetic dataset that mimics Normal + 6 attack types.\n",
    "    This lets the full pipeline run end-to-end even without real CSVs.\n",
    "    Replace this later with real merged IoT datasets.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "\n",
    "    n_classes = len(ATTACK_LABELS)  # 7 (Normal + 6 attacks)\n",
    "    total_samples = n_samples_per_class * n_classes\n",
    "\n",
    "    X = []\n",
    "    y_multi = []\n",
    "    attack_names = []\n",
    "\n",
    "    # Create a simple pattern per class to make the task non-trivial\n",
    "    for cls_id in range(n_classes):\n",
    "        # base mean vector differs per class\n",
    "        mean = rng.uniform(-2, 2, size=n_features) + cls_id * 0.5\n",
    "        cov = np.eye(n_features) * rng.uniform(0.5, 1.5)\n",
    "        samples = rng.multivariate_normal(\n",
    "            mean, cov, size=n_samples_per_class\n",
    "        )\n",
    "        X.append(samples)\n",
    "        y_multi.extend([cls_id] * n_samples_per_class)\n",
    "        attack_names.extend([ATTACK_ID_TO_LABEL[cls_id]] * n_samples_per_class)\n",
    "\n",
    "    X = np.vstack(X)\n",
    "    y_multi = np.array(y_multi, dtype=int)\n",
    "\n",
    "    # Binary labels: 0 = Normal, 1 = Attack\n",
    "    y_binary = (y_multi != ATTACK_LABEL_TO_ID[\"Normal\"]).astype(int)\n",
    "\n",
    "    feature_names = [f\"f_{i}\" for i in range(n_features)]\n",
    "    df_X = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "    df = df_X.copy()\n",
    "    df[\"label_binary\"] = y_binary\n",
    "    df[\"label_multi\"] = y_multi\n",
    "    df[\"attack_name\"] = attack_names\n",
    "\n",
    "    return {\n",
    "        \"dataframe\": df,\n",
    "        \"feature_names\": feature_names,\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# 5. BGWO Feature Selector\n",
    "# =========================\n",
    "\n",
    "class BGWOFeatureSelector:\n",
    "    \"\"\"\n",
    "    Binary Grey Wolf Optimizer for feature selection using XGBoost as evaluator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        max_iter: int = 15,\n",
    "        n_wolves: int = 8,\n",
    "        random_state: int = RANDOM_STATE,\n",
    "    ):\n",
    "        self.n_features = n_features\n",
    "        self.max_iter = max_iter\n",
    "        self.n_wolves = n_wolves\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.best_mask_: Optional[np.ndarray] = None\n",
    "        self.best_score_: float = -np.inf\n",
    "\n",
    "        self._rng = np.random.RandomState(self.random_state)\n",
    "\n",
    "    def _initialize_positions(self) -> np.ndarray:\n",
    "        # Continuous positions in [0,1] for each wolf and feature\n",
    "        return self._rng.uniform(0, 1, size=(self.n_wolves, self.n_features))\n",
    "\n",
    "    @staticmethod\n",
    "    def _continuous_to_binary(positions: np.ndarray) -> np.ndarray:\n",
    "        # Sigmoid + threshold to get binary mask\n",
    "        sigmoid = 1 / (1 + np.exp(-10 * (positions - 0.5)))\n",
    "        return (sigmoid >= 0.5).astype(int)\n",
    "\n",
    "    def _fitness(\n",
    "        self, mask: np.ndarray, X: np.ndarray, y: np.ndarray\n",
    "    ) -> float:\n",
    "        # If no features selected, return very poor score\n",
    "        if mask.sum() == 0:\n",
    "            return 0.0\n",
    "\n",
    "        X_sel = X[:, mask == 1]\n",
    "\n",
    "        # Lightweight XGBoost for evaluation\n",
    "        params = dict(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            objective=\"binary:logistic\"\n",
    "            if len(np.unique(y)) == 2\n",
    "            else \"multi:softprob\",\n",
    "            tree_method=\"hist\",\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        if len(np.unique(y)) > 2:\n",
    "            params[\"num_class\"] = len(np.unique(y))\n",
    "\n",
    "        cv = StratifiedKFold(\n",
    "            n_splits=3, shuffle=True, random_state=self.random_state\n",
    "        )\n",
    "        scores = []\n",
    "        for train_idx, valid_idx in cv.split(X_sel, y):\n",
    "            X_tr, X_va = X_sel[train_idx], X_sel[valid_idx]\n",
    "            y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "\n",
    "            model = XGBClassifier(**params)\n",
    "            model.fit(X_tr, y_tr)\n",
    "\n",
    "            if len(np.unique(y)) == 2:\n",
    "                probs = model.predict_proba(X_va)[:, 1]\n",
    "                score = roc_auc_score(y_va, probs)\n",
    "            else:\n",
    "                preds = model.predict(X_va)\n",
    "                score = (preds == y_va).mean()\n",
    "            scores.append(score)\n",
    "\n",
    "        return float(np.mean(scores))\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Run BGWO to select a subset of features. Returns indices of selected features.\n",
    "        \"\"\"\n",
    "        positions = self._initialize_positions()\n",
    "        binary_masks = self._continuous_to_binary(positions)\n",
    "\n",
    "        # Evaluate initial fitness\n",
    "        fitness = np.zeros(self.n_wolves)\n",
    "        for i in range(self.n_wolves):\n",
    "            fitness[i] = self._fitness(binary_masks[i], X, y)\n",
    "\n",
    "        # Identify alpha, beta, delta wolves (best three)\n",
    "        def get_leaders():\n",
    "            sorted_idx = np.argsort(-fitness)  # descending\n",
    "            return sorted_idx[0], sorted_idx[1], sorted_idx[2]\n",
    "\n",
    "        alpha_idx, beta_idx, delta_idx = get_leaders()\n",
    "\n",
    "        self.best_mask_ = binary_masks[alpha_idx].copy()\n",
    "        self.best_score_ = fitness[alpha_idx]\n",
    "\n",
    "        # Main loop\n",
    "        for iter_idx in range(self.max_iter):\n",
    "            a = 2 - 2 * (iter_idx / (self.max_iter - 1 + 1e-9))  # linearly from 2 to 0\n",
    "\n",
    "            for i in range(self.n_wolves):\n",
    "                for d in range(self.n_features):\n",
    "                    r1, r2 = self._rng.rand(), self._rng.rand()\n",
    "                    A1 = 2 * a * r1 - a\n",
    "                    C1 = 2 * r2\n",
    "\n",
    "                    r1, r2 = self._rng.rand(), self._rng.rand()\n",
    "                    A2 = 2 * a * r1 - a\n",
    "                    C2 = 2 * r2\n",
    "\n",
    "                    r1, r2 = self._rng.rand(), self._rng.rand()\n",
    "                    A3 = 2 * a * r1 - a\n",
    "                    C3 = 2 * r2\n",
    "\n",
    "                    X_alpha_d = positions[alpha_idx, d]\n",
    "                    X_beta_d = positions[beta_idx, d]\n",
    "                    X_delta_d = positions[delta_idx, d]\n",
    "\n",
    "                    D_alpha = abs(C1 * X_alpha_d - positions[i, d])\n",
    "                    D_beta = abs(C2 * X_beta_d - positions[i, d])\n",
    "                    D_delta = abs(C3 * X_delta_d - positions[i, d])\n",
    "\n",
    "                    X1 = X_alpha_d - A1 * D_alpha\n",
    "                    X2 = X_beta_d - A2 * D_beta\n",
    "                    X3 = X_delta_d - A3 * D_delta\n",
    "\n",
    "                    new_pos = (X1 + X2 + X3) / 3.0\n",
    "                    # Keep within [0,1]\n",
    "                    positions[i, d] = max(0.0, min(1.0, new_pos))\n",
    "\n",
    "            # Update binary masks and fitness\n",
    "            binary_masks = self._continuous_to_binary(positions)\n",
    "            for i in range(self.n_wolves):\n",
    "                fitness[i] = self._fitness(binary_masks[i], X, y)\n",
    "\n",
    "            alpha_idx, beta_idx, delta_idx = get_leaders()\n",
    "\n",
    "            if fitness[alpha_idx] > self.best_score_:\n",
    "                self.best_score_ = fitness[alpha_idx]\n",
    "                self.best_mask_ = binary_masks[alpha_idx].copy()\n",
    "\n",
    "            print(\n",
    "                f\"[BGWO] Iter {iter_idx+1}/{self.max_iter} - \"\n",
    "                f\"Best fitness: {self.best_score_:.4f}, \"\n",
    "                f\"Selected features: {int(self.best_mask_.sum())}\"\n",
    "            )\n",
    "\n",
    "        # Safety: if all-zero mask, fall back to top-k features by variance\n",
    "        if self.best_mask_.sum() == 0:\n",
    "            print(\"[BGWO] Warning: empty mask, falling back to variance-based selection.\")\n",
    "            variances = np.var(X, axis=0)\n",
    "            k = max(1, int(0.2 * self.n_features))\n",
    "            top_idx = np.argsort(-variances)[:k]\n",
    "            self.best_mask_[top_idx] = 1\n",
    "\n",
    "        selected_indices = np.where(self.best_mask_ == 1)[0]\n",
    "        return selected_indices\n",
    "\n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.best_mask_ is None:\n",
    "            raise RuntimeError(\"BGWOFeatureSelector must be fitted before transform().\")\n",
    "        return X[:, self.best_mask_ == 1]\n",
    "\n",
    "    def fit_transform(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n",
    "\n",
    "# =========================\n",
    "# 6. RFE with XGBoost (Stage 2)\n",
    "# =========================\n",
    "\n",
    "def rfe_xgboost(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    feature_indices: np.ndarray,\n",
    "    n_features_to_select: Optional[int] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply RFE on top of BGWO-selected features.\n",
    "    Returns final selected feature indices (referring to original feature space).\n",
    "    \"\"\"\n",
    "    if n_features_to_select is None:\n",
    "        n_features_to_select = max(1, int(len(feature_indices) * 0.5))\n",
    "\n",
    "    # Ensure numpy arrays\n",
    "    feature_indices = np.array(feature_indices, dtype=int)\n",
    "    X_sel = X[:, feature_indices]\n",
    "\n",
    "    # Basic XGBoost estimator for RFE\n",
    "    params = dict(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        objective=\"binary:logistic\"\n",
    "        if len(np.unique(y)) == 2\n",
    "        else \"multi:softprob\",\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    if len(np.unique(y)) > 2:\n",
    "        params[\"num_class\"] = len(np.unique(y))\n",
    "\n",
    "    estimator = XGBClassifier(**params)\n",
    "    rfe = RFE(\n",
    "        estimator=estimator,\n",
    "        n_features_to_select=n_features_to_select,\n",
    "        step=1,\n",
    "    )\n",
    "    rfe.fit(X_sel, y)\n",
    "\n",
    "    support_mask = rfe.support_\n",
    "    selected_within = np.where(support_mask)[0]\n",
    "    final_indices = feature_indices[selected_within]\n",
    "\n",
    "    print(\n",
    "        f\"[RFE] Reduced from {len(feature_indices)} to {len(final_indices)} features.\"\n",
    "    )\n",
    "    return final_indices\n",
    "\n",
    "# =========================\n",
    "# 7. Hyperparameter optimization with BO-TPE (Optuna)\n",
    "# =========================\n",
    "\n",
    "def optuna_objective(\n",
    "    trial: optuna.trial.Trial,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    task_type: str = \"binary\",\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Objective for Optuna: tune XGBoost hyperparameters.\n",
    "    BO-TPE is used via Optuna's TPESampler.\n",
    "    \"\"\"\n",
    "    n_classes = len(np.unique(y))\n",
    "\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 400),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\n",
    "            \"learning_rate\", 0.01, 0.3, log=True\n",
    "        ),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "\n",
    "    if task_type == \"binary\":\n",
    "        params[\"objective\"] = \"binary:logistic\"\n",
    "        eval_metric = \"auc\"\n",
    "    else:\n",
    "        params[\"objective\"] = \"multi:softprob\"\n",
    "        params[\"num_class\"] = n_classes\n",
    "        eval_metric = \"mlogloss\"\n",
    "\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=3, shuffle=True, random_state=RANDOM_STATE\n",
    "    )\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, valid_idx in cv.split(X, y):\n",
    "        X_tr, X_va = X[train_idx], X[valid_idx]\n",
    "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "\n",
    "        model = XGBClassifier(\n",
    "            **params,\n",
    "            eval_metric=eval_metric,\n",
    "        )\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        if task_type == \"binary\":\n",
    "            probs = model.predict_proba(X_va)[:, 1]\n",
    "            score = roc_auc_score(y_va, probs)\n",
    "        else:\n",
    "            preds = model.predict(X_va)\n",
    "            score = (preds == y_va).mean()\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "\n",
    "def tune_hyperparameters(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    task_type: str = \"binary\",\n",
    "    n_trials: int = 25,\n",
    "    study_name: str = \"xgb_optuna_study\",\n",
    ") -> Tuple[Dict, optuna.Study]:\n",
    "    \"\"\"\n",
    "    Run BO-TPE hyperparameter search with Optuna for XGBoost.\n",
    "    Returns best_params and study.\n",
    "    \"\"\"\n",
    "    sampler = TPESampler(seed=RANDOM_STATE)\n",
    "    direction = \"maximize\"\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        direction=direction,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "\n",
    "    study.optimize(\n",
    "        lambda trial: optuna_objective(\n",
    "            trial, X, y, task_type=task_type\n",
    "        ),\n",
    "        n_trials=n_trials,\n",
    "        show_progress_bar=False,\n",
    "    )\n",
    "\n",
    "    best_params = study.best_params.copy()\n",
    "    best_params.update(\n",
    "        {\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"n_jobs\": -1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if task_type == \"binary\":\n",
    "        best_params[\"objective\"] = \"binary:logistic\"\n",
    "    else:\n",
    "        best_params[\"objective\"] = \"multi:softprob\"\n",
    "        best_params[\"num_class\"] = len(np.unique(y))\n",
    "\n",
    "    print(\n",
    "        f\"[Optuna] Best score: {study.best_value:.4f} with params:\\n{json.dumps(best_params, indent=2)}\"\n",
    "    )\n",
    "    return best_params, study\n",
    "\n",
    "# =========================\n",
    "# 8. Attack-specific metrics\n",
    "# =========================\n",
    "\n",
    "def print_attack_specific_report(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    label_map: Dict[int, str],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Print and return a per-attack classification report.\n",
    "    \"\"\"\n",
    "    unique_labels = sorted(np.unique(y_true))\n",
    "    target_names = [label_map[i] for i in unique_labels]\n",
    "\n",
    "    report_dict = classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        target_names=target_names,\n",
    "        output_dict=True,\n",
    "        zero_division=0,\n",
    "    )\n",
    "    df_report = pd.DataFrame(report_dict).transpose()\n",
    "    print(\"\\n=== Attack-specific classification report ===\")\n",
    "    print(df_report[[\"precision\", \"recall\", \"f1-score\", \"support\"]])\n",
    "    return df_report\n",
    "\n",
    "# =========================\n",
    "# 9. Full BGWO-RFE-XGBoost pipeline\n",
    "# =========================\n",
    "\n",
    "def run_bgwo_rfe_xgb_pipeline(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols: List[str],\n",
    "    label_col: str,\n",
    "    task_type: str,\n",
    "    checkpoint_prefix: str,\n",
    "    label_map: Optional[Dict[int, str]] = None,\n",
    "    bgwo_max_iter: int = 10,\n",
    "    bgwo_n_wolves: int = 6,\n",
    "    n_optuna_trials: int = 15,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run the complete pipeline:\n",
    "      - Train/test split\n",
    "      - Scaling\n",
    "      - SMOTE\n",
    "      - BGWO (Stage 1)\n",
    "      - RFE-XGBoost (Stage 2)\n",
    "      - BO-TPE hyperparameter tuning\n",
    "      - Final training and evaluation\n",
    "      - Checkpoint saving\n",
    "    \"\"\"\n",
    "\n",
    "    assert task_type in (\"binary\", \"multiclass\")\n",
    "\n",
    "    X = df[feature_cols].values.astype(np.float32)\n",
    "    y = df[label_col].values.astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        stratify=y,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Handle class imbalance with SMOTE (on training data only)\n",
    "    smote = SMOTE(random_state=RANDOM_STATE)\n",
    "    X_train_bal, y_train_bal = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "    print(f\"[Data] Train shape (balanced): {X_train_bal.shape}\")\n",
    "    print(f\"[Data] Test shape: {X_test_scaled.shape}\")\n",
    "\n",
    "    # Stage 1: BGWO\n",
    "    bgwo = BGWOFeatureSelector(\n",
    "        n_features=X_train_bal.shape[1],\n",
    "        max_iter=bgwo_max_iter,\n",
    "        n_wolves=bgwo_n_wolves,\n",
    "    )\n",
    "    bgwo_selected_idx = bgwo.fit(X_train_bal, y_train_bal)\n",
    "    print(f\"[BGWO] Selected {len(bgwo_selected_idx)} features via BGWO.\")\n",
    "\n",
    "    # Stage 2: RFE\n",
    "    final_feature_idx = rfe_xgboost(\n",
    "        X_train_bal,\n",
    "        y_train_bal,\n",
    "        bgwo_selected_idx,\n",
    "        n_features_to_select=max(1, int(len(bgwo_selected_idx) * 0.5)),\n",
    "    )\n",
    "    print(f\"[RFE] Final selected features: {len(final_feature_idx)}\")\n",
    "\n",
    "    selected_feature_names = [feature_cols[i] for i in final_feature_idx]\n",
    "\n",
    "    # Reduce data to final selected features\n",
    "    X_train_fs = X_train_bal[:, final_feature_idx]\n",
    "    X_test_fs = X_test_scaled[:, final_feature_idx]\n",
    "\n",
    "    # Hyperparameter tuning with BO-TPE (Optuna)\n",
    "    best_params, study = tune_hyperparameters(\n",
    "        X_train_fs,\n",
    "        y_train_bal,\n",
    "        task_type=task_type,\n",
    "        n_trials=n_optuna_trials,\n",
    "        study_name=f\"{checkpoint_prefix}_optuna\",\n",
    "    )\n",
    "\n",
    "    # Final model training\n",
    "    model = XGBClassifier(**best_params)\n",
    "    model.fit(X_train_fs, y_train_bal)\n",
    "\n",
    "    # Evaluation\n",
    "    y_pred = model.predict(X_test_fs)\n",
    "\n",
    "    print(\"\\n=== Global classification report ===\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_test,\n",
    "            y_pred,\n",
    "            zero_division=0,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if task_type == \"binary\":\n",
    "        y_prob = model.predict_proba(X_test_fs)[:, 1]\n",
    "        try:\n",
    "            auc = roc_auc_score(y_test, y_prob)\n",
    "            print(f\"[Binary] Test ROC-AUC: {auc:.4f}\")\n",
    "        except ValueError:\n",
    "            print(\"[Binary] ROC-AUC could not be computed (single class in y_test).\")\n",
    "\n",
    "    # Attack-specific analysis only for multi-class\n",
    "    attack_report_df = None\n",
    "    if task_type == \"multiclass\" and label_map is not None:\n",
    "        attack_report_df = print_attack_specific_report(\n",
    "            y_true=y_test,\n",
    "            y_pred=y_pred,\n",
    "            label_map=label_map,\n",
    "        )\n",
    "\n",
    "    # Confusion matrix\n",
    "    print(\"\\n=== Confusion matrix ===\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "    # Save checkpoints\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "    model_path = os.path.join(\n",
    "        CHECKPOINT_DIR, f\"{checkpoint_prefix}_xgb_model.pkl\"\n",
    "    )\n",
    "    scaler_path = os.path.join(\n",
    "        CHECKPOINT_DIR, f\"{checkpoint_prefix}_scaler.pkl\"\n",
    "    )\n",
    "    feature_idx_path = os.path.join(\n",
    "        CHECKPOINT_DIR, f\"{checkpoint_prefix}_feature_indices.pkl\"\n",
    "    )\n",
    "    feature_names_path = os.path.join(\n",
    "        CHECKPOINT_DIR, f\"{checkpoint_prefix}_feature_names.json\"\n",
    "    )\n",
    "    study_path = os.path.join(\n",
    "        CHECKPOINT_DIR, f\"{checkpoint_prefix}_optuna_study.pkl\"\n",
    "    )\n",
    "\n",
    "    joblib.dump(model, model_path)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    joblib.dump(final_feature_idx, feature_idx_path)\n",
    "    with open(feature_names_path, \"w\") as f:\n",
    "        json.dump(selected_feature_names, f, indent=2)\n",
    "    joblib.dump(study, study_path)\n",
    "\n",
    "    print(f\"\\n[Checkpoint] Saved model to: {model_path}\")\n",
    "    print(f\"[Checkpoint] Saved scaler to: {scaler_path}\")\n",
    "    print(f\"[Checkpoint] Saved feature indices to: {feature_idx_path}\")\n",
    "    print(f\"[Checkpoint] Saved feature names to: {feature_names_path}\")\n",
    "    print(f\"[Checkpoint] Saved Optuna study to: {study_path}\")\n",
    "\n",
    "    results = {\n",
    "        \"model\": model,\n",
    "        \"scaler\": scaler,\n",
    "        \"selected_feature_indices\": final_feature_idx,\n",
    "        \"selected_feature_names\": selected_feature_names,\n",
    "        \"study\": study,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"attack_report_df\": attack_report_df,\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# =========================\n",
    "# 10. Example: run pipeline on synthetic IoT dataset\n",
    "# =========================\n",
    "\n",
    "# Build synthetic dataset that mimics:\n",
    "# - Normal traffic\n",
    "# - Data Tampering\n",
    "# - Impersonation/Spoofing\n",
    "# - DDoS/DoS\n",
    "# - Silent Exfiltration\n",
    "# - Backdoor\n",
    "# - Reconnaissance\n",
    "\n",
    "synthetic_data = build_synthetic_iot_dataset(\n",
    "    n_samples_per_class=400,  # reduce/increase for speed vs. accuracy\n",
    "    n_features=32,\n",
    ")\n",
    "df_syn = synthetic_data[\"dataframe\"]\n",
    "feature_cols_syn = synthetic_data[\"feature_names\"]\n",
    "\n",
    "print(\"Synthetic dataset shape:\", df_syn.shape)\n",
    "print(\"Class distribution (multi):\")\n",
    "print(df_syn[\"label_multi\"].value_counts().sort_index())\n",
    "\n",
    "# ---- Binary classification: Normal vs Attack ----\n",
    "binary_results = run_bgwo_rfe_xgb_pipeline(\n",
    "    df=df_syn,\n",
    "    feature_cols=feature_cols_syn,\n",
    "    label_col=\"label_binary\",\n",
    "    task_type=\"binary\",\n",
    "    checkpoint_prefix=\"synthetic_binary\",\n",
    "    label_map=None,\n",
    "    bgwo_max_iter=5,     # small for demo; increase for better performance\n",
    "    bgwo_n_wolves=5,\n",
    "    n_optuna_trials=10,\n",
    ")\n",
    "\n",
    "# ---- Multi-class classification: 7-class attack types ----\n",
    "multi_results = run_bgwo_rfe_xgb_pipeline(\n",
    "    df=df_syn,\n",
    "    feature_cols=feature_cols_syn,\n",
    "    label_col=\"label_multi\",\n",
    "    task_type=\"multiclass\",\n",
    "    checkpoint_prefix=\"synthetic_multiclass\",\n",
    "    label_map=ATTACK_ID_TO_LABEL,\n",
    "    bgwo_max_iter=5,\n",
    "    bgwo_n_wolves=5,\n",
    "    n_optuna_trials=10,\n",
    ")\n",
    "\n",
    "print(\"\\nPipeline execution on synthetic data is complete.\")\n",
    "print(\"You can now plug in your real merged IoT datasets in place of df_syn.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
